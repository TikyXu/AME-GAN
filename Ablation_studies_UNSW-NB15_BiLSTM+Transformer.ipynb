{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from einops import repeat, pack\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "# device_dis = torch.device('cuda:0')\n",
    "# device_gen = torch.device('cuda:0')\n",
    "\n",
    "# 获取当前日期和时间\n",
    "run_time = datetime.now()\n",
    "# model_kind = f'BiLSTM'\n",
    "model_kind = f'BiLSTM'\n",
    "\n",
    "save_path = os.path.join(f'ablation_study_results', str(run_time))\n",
    "save_path = os.path.join(save_path, model_kind)\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(f'{save_path}', exist_ok=True)\n",
    "os.makedirs(f'{save_path}/models', exist_ok=True)\n",
    "os.makedirs(f'{save_path}/results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_UNSWNB15(path_train_data, path_test_data, binary_or_multi='multi'):\n",
    "\n",
    "    categorical_columns = ['proto', 'service', 'state']\n",
    "    \n",
    "    classification = ['Normal', 'Fuzzers', 'Analysis', 'Backdoor', 'DoS', \n",
    "                    'Exploits', 'Generic', 'Reconnaissance', 'Shellcode', 'Worms']\n",
    "    # 加载数据 train_num:125973, test_num:22544, total_data:148517\n",
    "    data_train = pd.read_csv(path_train_data).copy()\n",
    "    data_test = pd.read_csv(path_test_data).copy()\n",
    "    total_data = pd.concat([data_train, data_test], axis=0) # 合并train和test\n",
    "    total_data = total_data.drop(['id'], axis=1)\n",
    "    train_num = len(data_train)\n",
    "    test_num = len(data_test)\n",
    "\n",
    "    # 特征\n",
    "    features = total_data.iloc[:, :-2]     \n",
    "    \n",
    "    # 标签（以Binary/Multi形式加载Y的值）\n",
    "    if binary_or_multi=='binary':    \n",
    "        # 删除attack_cat列\n",
    "        total_data = total_data.drop('attack_cat', axis=1)\n",
    "        # 把labels转换为binary[0,1] \n",
    "        labels = total_data.iloc[:, -1]\n",
    "    elif binary_or_multi=='multi':\n",
    "        # 删除label列\n",
    "        total_data = total_data.drop('label', axis=1)\n",
    "        labels_class = total_data.iloc[:, -1]\n",
    "        \n",
    "        pdlist_class_dict = {}\n",
    "        for index, data_class in enumerate(classification):\n",
    "            pdlist_class_dict[data_class] = index\n",
    "                \n",
    "        labels = np.array([pdlist_class_dict[row] for row in np.array(labels_class)])\n",
    "        \n",
    "    # One-hot编码数据\n",
    "    features = pd.get_dummies(features, columns=categorical_columns)\n",
    "    \n",
    "    # Min-Max标准化\n",
    "    scaler = MinMaxScaler().fit(features)\n",
    "    features = scaler.transform(features)\n",
    "\n",
    "    # 凑形状，增加60列\n",
    "    addition_number = 60\n",
    "    addition_data = np.zeros((len(total_data), addition_number))\n",
    "    features = np.concatenate((features, addition_data), axis=1)\n",
    "    \n",
    "    # X_train = features[:train_num][:, :, np.newaxis]\n",
    "    # X_test = features[train_num:][:, :, np.newaxis]\n",
    "    X_train = features[:train_num].astype(np.float32)\n",
    "    X_test = features[train_num:].astype(np.float32)\n",
    "    Y_train = labels[:train_num].astype(np.longlong)\n",
    "    Y_test = labels[train_num:].astype(np.longlong)\n",
    "    \n",
    "        \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_train = torch.LongTensor(Y_train)\n",
    "    Y_test = torch.LongTensor(Y_test)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_classes, dim, dropout):\n",
    "        super().__init__()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Linear(dim, num_classes),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_head(x)\n",
    "    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, *, seq_len, patch_size, dim, channels, emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert (seq_len % patch_size) == 0\n",
    "\n",
    "        num_patches = seq_len // patch_size\n",
    "        patch_dim = channels * patch_size\n",
    "        # patch_dim = patch_size\n",
    "        self.patch_dim = [patch_size, channels, patch_dim]\n",
    "        \n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (n p) -> b n (p c)', p = patch_size),\n",
    "            # batch_size channels (patch_number * patch_size) -> batch_size patch_number (patch_size * channels)\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "    def forward(self, series):\n",
    "        x = self.to_patch_embedding(series)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, 'd -> b d', b = b)\n",
    "        \n",
    "        x, ps = pack([cls_tokens, x], 'b * d')\n",
    "\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x, ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, lstm_hidden_dim=64, lstm_layers=2, dropout=0.1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_dim, lstm_hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.embedding = nn.Linear(lstm_hidden_dim*2, int(input_dim/2))\n",
    "        \n",
    "        self.mlp = MultiLayerPerceptron(dim=int(input_dim/2),\n",
    "                                        num_classes=num_classes,\n",
    "                                        dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM部分\n",
    "        x, _ = self.bilstm(x)  # x: (batch_size, seq_length, input_dim)\n",
    "        x = x[:, -1, :]  # 取最后一个时间步的输出作为特征\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        y = self.mlp(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### +Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTransformer(nn.Module):    \n",
    "    def __init__(self, input_dim, num_classes, lstm_hidden_dim=64, lstm_layers=2, nhead=4, dim_feedforward=128, num_layers=6, dropout=0.1):\n",
    "        super(BiLSTMTransformer, self).__init__()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_dim, lstm_hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(lstm_hidden_dim*2, int(input_dim/2))\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=int(input_dim/2), nhead=nhead, dim_feedforward=int(input_dim/2), dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.mlp = MultiLayerPerceptron(dim=int(input_dim/2),\n",
    "                                        num_classes=num_classes,\n",
    "                                        dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM部分\n",
    "        x, _ = self.bilstm(x)  # x: (batch_size, seq_length, input_dim)\n",
    "        x = x[:, -1, :]  # 取最后一个时间步的输出作为特征\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        y = self.mlp(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数、模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoches = 30\n",
    "batch_size = 256\n",
    "last_epoch = -1\n",
    "print_interval = 100\n",
    "\n",
    "input_dim = 256\n",
    "class_num = 10\n",
    "lstm_layers = 2\n",
    "nhead = 4\n",
    "tb_layers = 4\n",
    "dropout = 0.1\n",
    "\n",
    "# model = BiLSTM(input_dim=input_dim, \n",
    "#                num_classes=class_num, \n",
    "#                lstm_hidden_dim=input_dim, \n",
    "#                lstm_layers=lstm_layers,\n",
    "#                dropout=dropout).to(torch.device(\"cuda\"))\n",
    "\n",
    "model = BiLSTMTransformer(input_dim=input_dim, \n",
    "                          num_classes=class_num, \n",
    "                          lstm_hidden_dim=input_dim, \n",
    "                          lstm_layers=lstm_layers, \n",
    "                          nhead=nhead, \n",
    "                          dim_feedforward=input_dim, \n",
    "                          num_layers=tb_layers, \n",
    "                          dropout=dropout).to(torch.device(\"cuda\"))\n",
    "# model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_model = Adam(params=model.parameters(), lr=1e-3)\n",
    "scheduler_model = CosineAnnealingLR(optimizer_model, T_max=train_epoches, last_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "path_train_data='datasets/UNSW-NB15/UNSW_NB15_training-set.csv'\n",
    "path_test_data='datasets/UNSW-NB15/UNSW_NB15_testing-set.csv'\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = Load_UNSWNB15(path_train_data=path_train_data, \n",
    "                                                 path_test_data=path_test_data, \n",
    "                                                 binary_or_multi='multi')# 装载数据到loader里面\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch1\n",
      "Epo0/Iter100:Accu 0.6953125\n",
      "Epo0/Iter200:Accu 0.734375\n",
      "Epo0/Iter300:Accu 0.67578125\n",
      "Epo0/Iter400:Accu 0.73046875\n",
      "Epo0/Iter500:Accu 0.73828125\n",
      "Epo0/Iter600:Accu 0.69921875\n",
      "Epoch 1/30:\n",
      "Accuracy:0.6164674731574601\n",
      "Best Accuracy:0.6164674731574601 - Epoch:1\n",
      "\n",
      "Training Epoch2\n",
      "Epo1/Iter100:Accu 0.73828125\n",
      "Epo1/Iter200:Accu 0.671875\n",
      "Epo1/Iter300:Accu 0.72265625\n",
      "Epo1/Iter400:Accu 0.73046875\n",
      "Epo1/Iter500:Accu 0.75\n",
      "Epo1/Iter600:Accu 0.734375\n",
      "Epoch 2/30:\n",
      "Accuracy:0.6543749696351358\n",
      "Best Accuracy:0.6543749696351358 - Epoch:2\n",
      "\n",
      "Training Epoch3\n",
      "Epo2/Iter100:Accu 0.72265625\n",
      "Epo2/Iter200:Accu 0.72265625\n",
      "Epo2/Iter300:Accu 0.70703125\n",
      "Epo2/Iter400:Accu 0.73046875\n",
      "Epo2/Iter500:Accu 0.796875\n",
      "Epo2/Iter600:Accu 0.76171875\n",
      "Epoch 3/30:\n",
      "Accuracy:0.6380508186367391\n",
      "Best Accuracy:0.6543749696351358 - Epoch:2\n",
      "\n",
      "Training Epoch4\n",
      "Epo3/Iter100:Accu 0.67578125\n",
      "Epo3/Iter200:Accu 0.76171875\n",
      "Epo3/Iter300:Accu 0.75390625\n",
      "Epo3/Iter400:Accu 0.71875\n",
      "Epo3/Iter500:Accu 0.72265625\n",
      "Epo3/Iter600:Accu 0.75\n",
      "Epoch 4/30:\n",
      "Accuracy:0.5929529223145314\n",
      "Best Accuracy:0.6543749696351358 - Epoch:2\n",
      "\n",
      "Training Epoch5\n",
      "Epo4/Iter100:Accu 0.7421875\n",
      "Epo4/Iter200:Accu 0.75\n",
      "Epo4/Iter300:Accu 0.75390625\n",
      "Epo4/Iter400:Accu 0.76171875\n",
      "Epo4/Iter500:Accu 0.75390625\n",
      "Epo4/Iter600:Accu 0.72265625\n",
      "Epoch 5/30:\n",
      "Accuracy:0.6435529320312879\n",
      "Best Accuracy:0.6543749696351358 - Epoch:2\n",
      "\n",
      "Training Epoch6\n",
      "Epo5/Iter100:Accu 0.734375\n",
      "Epo5/Iter200:Accu 0.72265625\n",
      "Epo5/Iter300:Accu 0.76953125\n",
      "Epo5/Iter400:Accu 0.6875\n",
      "Epo5/Iter500:Accu 0.76171875\n",
      "Epo5/Iter600:Accu 0.78515625\n",
      "Epoch 6/30:\n",
      "Accuracy:0.6624763154059176\n",
      "Best Accuracy:0.6624763154059176 - Epoch:6\n",
      "\n",
      "Training Epoch7\n",
      "Epo6/Iter100:Accu 0.79296875\n",
      "Epo6/Iter200:Accu 0.7421875\n",
      "Epo6/Iter300:Accu 0.7734375\n",
      "Epo6/Iter400:Accu 0.72265625\n",
      "Epo6/Iter500:Accu 0.7265625\n",
      "Epo6/Iter600:Accu 0.71484375\n",
      "Epoch 7/30:\n",
      "Accuracy:0.6739299421852986\n",
      "Best Accuracy:0.6739299421852986 - Epoch:7\n",
      "\n",
      "Training Epoch8\n",
      "Epo7/Iter100:Accu 0.765625\n",
      "Epo7/Iter200:Accu 0.796875\n",
      "Epo7/Iter300:Accu 0.76171875\n",
      "Epo7/Iter400:Accu 0.73828125\n",
      "Epo7/Iter500:Accu 0.76171875\n",
      "Epo7/Iter600:Accu 0.7421875\n",
      "Epoch 8/30:\n",
      "Accuracy:0.6993878443375601\n",
      "Best Accuracy:0.6993878443375601 - Epoch:8\n",
      "\n",
      "Training Epoch9\n",
      "Epo8/Iter100:Accu 0.78515625\n",
      "Epo8/Iter200:Accu 0.77734375\n",
      "Epo8/Iter300:Accu 0.765625\n",
      "Epo8/Iter400:Accu 0.7421875\n",
      "Epo8/Iter500:Accu 0.78515625\n",
      "Epo8/Iter600:Accu 0.7265625\n",
      "Epoch 9/30:\n",
      "Accuracy:0.6328644998299567\n",
      "Best Accuracy:0.6993878443375601 - Epoch:8\n",
      "\n",
      "Training Epoch10\n",
      "Epo9/Iter100:Accu 0.75390625\n",
      "Epo9/Iter200:Accu 0.75390625\n",
      "Epo9/Iter300:Accu 0.73828125\n",
      "Epo9/Iter400:Accu 0.73828125\n",
      "Epo9/Iter500:Accu 0.765625\n",
      "Epo9/Iter600:Accu 0.74609375\n",
      "Epoch 10/30:\n",
      "Accuracy:0.644767526599621\n",
      "Best Accuracy:0.6993878443375601 - Epoch:8\n",
      "\n",
      "Training Epoch11\n",
      "Epo10/Iter100:Accu 0.765625\n",
      "Epo10/Iter200:Accu 0.76953125\n",
      "Epo10/Iter300:Accu 0.75390625\n",
      "Epo10/Iter400:Accu 0.75390625\n",
      "Epo10/Iter500:Accu 0.734375\n",
      "Epo10/Iter600:Accu 0.77734375\n",
      "Epoch 11/30:\n",
      "Accuracy:0.696885779526794\n",
      "Best Accuracy:0.6993878443375601 - Epoch:8\n",
      "\n",
      "Training Epoch12\n",
      "Epo11/Iter100:Accu 0.8046875\n",
      "Epo11/Iter200:Accu 0.78125\n",
      "Epo11/Iter300:Accu 0.75390625\n",
      "Epo11/Iter400:Accu 0.7578125\n",
      "Epo11/Iter500:Accu 0.71875\n",
      "Epo11/Iter600:Accu 0.76953125\n",
      "Epoch 12/30:\n",
      "Accuracy:0.7289753680221542\n",
      "Best Accuracy:0.7289753680221542 - Epoch:12\n",
      "\n",
      "Training Epoch13\n",
      "Epo12/Iter100:Accu 0.6953125\n",
      "Epo12/Iter200:Accu 0.75390625\n",
      "Epo12/Iter300:Accu 0.703125\n",
      "Epo12/Iter400:Accu 0.74609375\n",
      "Epo12/Iter500:Accu 0.75390625\n",
      "Epo12/Iter600:Accu 0.75390625\n",
      "Epoch 13/30:\n",
      "Accuracy:0.718396249331973\n",
      "Best Accuracy:0.7289753680221542 - Epoch:12\n",
      "\n",
      "Training Epoch14\n",
      "Epo13/Iter100:Accu 0.71875\n",
      "Epo13/Iter200:Accu 0.7890625\n",
      "Epo13/Iter300:Accu 0.76171875\n",
      "Epo13/Iter400:Accu 0.75\n",
      "Epo13/Iter500:Accu 0.7109375\n",
      "Epo13/Iter600:Accu 0.77734375\n",
      "Epoch 14/30:\n",
      "Accuracy:0.70004372540446\n",
      "Best Accuracy:0.7289753680221542 - Epoch:12\n",
      "\n",
      "Training Epoch15\n",
      "Epo14/Iter100:Accu 0.78125\n",
      "Epo14/Iter200:Accu 0.77734375\n",
      "Epo14/Iter300:Accu 0.7265625\n",
      "Epo14/Iter400:Accu 0.76171875\n",
      "Epo14/Iter500:Accu 0.76171875\n",
      "Epo14/Iter600:Accu 0.734375\n",
      "Epoch 15/30:\n",
      "Accuracy:0.6830029636107467\n",
      "Best Accuracy:0.7289753680221542 - Epoch:12\n",
      "\n",
      "Training Epoch16\n",
      "Epo15/Iter100:Accu 0.76953125\n",
      "Epo15/Iter200:Accu 0.76171875\n",
      "Epo15/Iter300:Accu 0.78515625\n",
      "Epo15/Iter400:Accu 0.7578125\n",
      "Epo15/Iter500:Accu 0.734375\n",
      "Epo15/Iter600:Accu 0.7890625\n",
      "Epoch 16/30:\n",
      "Accuracy:0.7079142982072584\n",
      "Best Accuracy:0.7289753680221542 - Epoch:12\n",
      "\n",
      "Training Epoch17\n",
      "Epo16/Iter100:Accu 0.82421875\n",
      "Epo16/Iter200:Accu 0.73828125\n",
      "Epo16/Iter300:Accu 0.73046875\n",
      "Epo16/Iter400:Accu 0.7421875\n",
      "Epo16/Iter500:Accu 0.75390625\n",
      "Epo16/Iter600:Accu 0.78125\n",
      "Epoch 17/30:\n",
      "Accuracy:0.7475343730262838\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch18\n",
      "Epo17/Iter100:Accu 0.77734375\n",
      "Epo17/Iter200:Accu 0.7890625\n",
      "Epo17/Iter300:Accu 0.796875\n",
      "Epo17/Iter400:Accu 0.796875\n",
      "Epo17/Iter500:Accu 0.73828125\n",
      "Epo17/Iter600:Accu 0.76171875\n",
      "Epoch 18/30:\n",
      "Accuracy:0.7263882815916047\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch19\n",
      "Epo18/Iter100:Accu 0.7578125\n",
      "Epo18/Iter200:Accu 0.74609375\n",
      "Epo18/Iter300:Accu 0.73828125\n",
      "Epo18/Iter400:Accu 0.8359375\n",
      "Epo18/Iter500:Accu 0.734375\n",
      "Epo18/Iter600:Accu 0.71484375\n",
      "Epoch 19/30:\n",
      "Accuracy:0.7272020599523878\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch20\n",
      "Epo19/Iter100:Accu 0.73828125\n",
      "Epo19/Iter200:Accu 0.7890625\n",
      "Epo19/Iter300:Accu 0.76171875\n",
      "Epo19/Iter400:Accu 0.71875\n",
      "Epo19/Iter500:Accu 0.79296875\n",
      "Epo19/Iter600:Accu 0.765625\n",
      "Epoch 20/30:\n",
      "Accuracy:0.6995457416314435\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch21\n",
      "Epo20/Iter100:Accu 0.78125\n",
      "Epo20/Iter200:Accu 0.77734375\n",
      "Epo20/Iter300:Accu 0.80859375\n",
      "Epo20/Iter400:Accu 0.79296875\n",
      "Epo20/Iter500:Accu 0.80078125\n",
      "Epo20/Iter600:Accu 0.78515625\n",
      "Epoch 21/30:\n",
      "Accuracy:0.7259631734926881\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch22\n",
      "Epo21/Iter100:Accu 0.703125\n",
      "Epo21/Iter200:Accu 0.7421875\n",
      "Epo21/Iter300:Accu 0.82421875\n",
      "Epo21/Iter400:Accu 0.77734375\n",
      "Epo21/Iter500:Accu 0.765625\n",
      "Epo21/Iter600:Accu 0.734375\n",
      "Epoch 22/30:\n",
      "Accuracy:0.7056551523101589\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch23\n",
      "Epo22/Iter100:Accu 0.74609375\n",
      "Epo22/Iter200:Accu 0.72265625\n",
      "Epo22/Iter300:Accu 0.7421875\n",
      "Epo22/Iter400:Accu 0.76171875\n",
      "Epo22/Iter500:Accu 0.7109375\n",
      "Epo22/Iter600:Accu 0.765625\n",
      "Epoch 23/30:\n",
      "Accuracy:0.7234611086819219\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch24\n",
      "Epo23/Iter100:Accu 0.7265625\n",
      "Epo23/Iter200:Accu 0.73046875\n",
      "Epo23/Iter300:Accu 0.796875\n",
      "Epo23/Iter400:Accu 0.74609375\n",
      "Epo23/Iter500:Accu 0.72265625\n",
      "Epo23/Iter600:Accu 0.7734375\n",
      "Epoch 24/30:\n",
      "Accuracy:0.7058859252781422\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch25\n",
      "Epo24/Iter100:Accu 0.7734375\n",
      "Epo24/Iter200:Accu 0.75390625\n",
      "Epo24/Iter300:Accu 0.75390625\n",
      "Epo24/Iter400:Accu 0.734375\n",
      "Epo24/Iter500:Accu 0.78515625\n",
      "Epo24/Iter600:Accu 0.7578125\n",
      "Epoch 25/30:\n",
      "Accuracy:0.7276393139969878\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch26\n",
      "Epo25/Iter100:Accu 0.796875\n",
      "Epo25/Iter200:Accu 0.81640625\n",
      "Epo25/Iter300:Accu 0.73046875\n",
      "Epo25/Iter400:Accu 0.7421875\n",
      "Epo25/Iter500:Accu 0.765625\n",
      "Epo25/Iter600:Accu 0.765625\n",
      "Epoch 26/30:\n",
      "Accuracy:0.7427610163727347\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch27\n",
      "Epo26/Iter100:Accu 0.75390625\n",
      "Epo26/Iter200:Accu 0.80078125\n",
      "Epo26/Iter300:Accu 0.75390625\n",
      "Epo26/Iter400:Accu 0.73828125\n",
      "Epo26/Iter500:Accu 0.81640625\n",
      "Epo26/Iter600:Accu 0.78515625\n",
      "Epoch 27/30:\n",
      "Accuracy:0.7437691298644512\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch28\n",
      "Epo27/Iter100:Accu 0.76171875\n",
      "Epo27/Iter200:Accu 0.7890625\n",
      "Epo27/Iter300:Accu 0.73828125\n",
      "Epo27/Iter400:Accu 0.75390625\n",
      "Epo27/Iter500:Accu 0.76171875\n",
      "Epo27/Iter600:Accu 0.75\n",
      "Epoch 28/30:\n",
      "Accuracy:0.7392508380702522\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch29\n",
      "Epo28/Iter100:Accu 0.796875\n",
      "Epo28/Iter200:Accu 0.73046875\n",
      "Epo28/Iter300:Accu 0.76171875\n",
      "Epo28/Iter400:Accu 0.73046875\n",
      "Epo28/Iter500:Accu 0.80859375\n",
      "Epo28/Iter600:Accu 0.77734375\n",
      "Epoch 29/30:\n",
      "Accuracy:0.7173152601661565\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n",
      "Training Epoch30\n",
      "Epo29/Iter100:Accu 0.78515625\n",
      "Epo29/Iter200:Accu 0.7734375\n",
      "Epo29/Iter300:Accu 0.78125\n",
      "Epo29/Iter400:Accu 0.79296875\n",
      "Epo29/Iter500:Accu 0.80859375\n",
      "Epo29/Iter600:Accu 0.7734375\n",
      "Epoch 30/30:\n",
      "Accuracy:0.7254530437739882\n",
      "Best Accuracy:0.7475343730262838 - Epoch:17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accu = 0\n",
    "best_epoch = 0\n",
    "for epo in range(train_epoches):\n",
    "    print(f'Training Epoch{epo+1}')\n",
    "    \n",
    "    model.train()\n",
    "    for index, (train_image, train_label) in enumerate(train_loader):\n",
    "        train_image, train_label = train_image.view(train_image.shape[0], -1, train_image.shape[1]).cuda(non_blocking=True), train_label.to(torch.int64).cuda(non_blocking=True)\n",
    "        train_label = torch.nn.functional.one_hot(train_label, class_num).to(torch.float32)\n",
    "        optimizer_model.zero_grad()\n",
    "        \n",
    "        outs_real = model(train_image).view(train_label.shape)\n",
    "        \n",
    "        dis_loss = criterion(outs_real, train_label)\n",
    "        \n",
    "        dis_loss.backward()\n",
    "        optimizer_model.step()\n",
    "        scheduler_model.step()\n",
    "        \n",
    "        correct = (torch.argmax(outs_real, dim=1) == torch.argmax(train_label, dim=1)).sum().item()\n",
    "        if (index % print_interval) == 0 and index != 0:\n",
    "            print(f'Epo{epo}/Iter{index}:Accu {correct/len(train_label)}')\n",
    "\n",
    "    model.eval()    \n",
    "    \n",
    "    Y_pred = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    for image, label in test_loader:\n",
    "        Y_test = np.append(Y_test, label, axis=None)\n",
    "        image, label = image.view(image.shape[0], -1, image.shape[1]).cuda(non_blocking=True), label.cuda(non_blocking=True)\n",
    "\n",
    "        predicted = model(image).view(label.shape[0], -1)\n",
    "        predicted = torch.argmax(predicted, dim=1)\n",
    "        \n",
    "        Y_pred = np.append(Y_pred, predicted.cpu().numpy(), axis=None)\n",
    "            \n",
    "    accuracy_test = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    if accuracy_test > best_accu:\n",
    "        best_accu = accuracy_test\n",
    "        best_epoch = epo\n",
    "        best_pred = Y_pred\n",
    "        ground_truth = Y_test\n",
    "        \n",
    "        #保存整个模型\n",
    "        torch.save(model, f'{save_path}/models/{model_kind}_{epo}_accu_{accuracy_test:.4f}.pth')\n",
    "        np.save(f'{save_path}/models/best_pred_accu_{accuracy_test:.4f}.npy', best_pred)\n",
    "        np.save(f'{save_path}/models/ground_truth_accu_{accuracy_test:.4f}.npy', ground_truth)\n",
    "        \n",
    "    print_lines = f'Epoch {epo+1}/{train_epoches}:\\nAccuracy:{accuracy_test}\\n'\n",
    "    print_lines += f'Best Accuracy:{best_accu} - Epoch:{best_epoch+1}\\n'\n",
    "    print(print_lines)\n",
    "    \n",
    "    with open(f'{save_path}/results/{model_kind}_Multi.txt', 'a', encoding='utf-8') as file:\n",
    "        # 将输出内容写入文件\n",
    "        file.write(print_lines+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
