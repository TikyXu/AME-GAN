{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "device_dis = torch.device('cuda:0')\n",
    "# device_gen = torch.device('cuda:0')\n",
    "\n",
    "# 获取当前日期和时间\n",
    "run_time = datetime.now()\n",
    "\n",
    "save_path = os.path.join(f'ablation_study_results', str(run_time))\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(f'{save_path}/Inversely-proportional_Generation', exist_ok=True)\n",
    "os.makedirs(f'{save_path}/Inversely-proportional_Generation/models', exist_ok=True)\n",
    "os.makedirs(f'{save_path}/Inversely-proportional_Generation/results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_NSLKDD(path_train_data, path_test_data, binary_or_multi='multi'):\n",
    "    # 列名，根据NSL-KDD数据集文档定义\n",
    "    column_names = [\n",
    "        \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "        \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
    "        \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
    "        \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "        \"is_hot_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
    "        \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
    "        \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "        \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "        \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "        \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\", \"score\"\n",
    "        # \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"\n",
    "    ]\n",
    "    normal = ['normal']    \n",
    "    dos    = ['back', 'land', 'neptune', 'pod', 'smurf', \n",
    "            'teardrop', 'apache2', 'mailbomb', 'processtable', 'udpstorm']\n",
    "    probe  = ['ipsweep', 'nmap', 'portsweep', 'satan', 'mscan', 'saint']\n",
    "    r2l    = ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'phf', \n",
    "            'spy', 'warezclient', 'warezmaster', 'sendmail', 'named', \n",
    "            'snmpgetattack', 'snmpguess', 'xlock', 'xsnoop', 'worm']\n",
    "    u2l    = ['buffer_overflow', 'loadmodule', 'perl', 'rootkit', 'httptunnel', \n",
    "            'ps', 'sqlattack', 'xterm']\n",
    "    \n",
    "    categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "        \n",
    "    # 加载数据 train_num:125973, test_num:22544, total_data:148517\n",
    "    data_train = pd.read_csv(path_train_data, header=None, names=column_names)\n",
    "    data_test = pd.read_csv(path_test_data, header=None, names=column_names)\n",
    "    total_data = pd.concat([data_train, data_test], axis=0) # 合并train和test\n",
    "    train_num = len(data_train)\n",
    "    # 删除Score列\n",
    "    total_data = total_data.drop('score', axis=1)\n",
    "\n",
    "    # 特征、标签\n",
    "    features = total_data.iloc[:, :-1] \n",
    "    labels = total_data.iloc[:, -1]\n",
    "    \n",
    "    # One-hot编码数据\n",
    "    features = pd.get_dummies(features, columns=categorical_columns)\n",
    "    \n",
    "    # Min-Max标准化\n",
    "    scaler = MinMaxScaler().fit(features)\n",
    "    features = scaler.transform(features)\n",
    "    \n",
    "    add_column_num = 6\n",
    "    add_column = np.zeros(shape=(features.shape[0], add_column_num))\n",
    "    features = np.concatenate((features, add_column), axis=1)\n",
    "\n",
    "    pdlist_class_dict = {}\n",
    "    for index, data_class in enumerate([normal, dos, probe, r2l, u2l]):\n",
    "        for item in data_class:\n",
    "            pdlist_class_dict[item] = index\n",
    "\n",
    "    # 给表格数据赋值\n",
    "    if binary_or_multi == 'multi':\n",
    "        labels = np.array([pdlist_class_dict[row] for row in labels])\n",
    "    elif binary_or_multi == 'binary':\n",
    "        labels = np.array([0 if row=='normal' else 1 for row in labels])\n",
    "    \n",
    "    X_train = np.array(features[:train_num]).astype(np.float32)\n",
    "    X_test = np.array(features[train_num:]).astype(np.float32)\n",
    "    Y_train = np.array(labels[:train_num]).astype(np.longlong)\n",
    "    Y_test = np.array(labels[train_num:]).astype(np.longlong)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_train = torch.LongTensor(Y_train)\n",
    "    Y_test = torch.LongTensor(Y_test)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_classes, dim, dropout):\n",
    "        super().__init__()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Linear(dim, num_classes),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_head(x)\n",
    "    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, *, seq_len, patch_size, dim, channels, emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert (seq_len % patch_size) == 0\n",
    "\n",
    "        num_patches = seq_len // patch_size\n",
    "        patch_dim = channels * patch_size\n",
    "        # patch_dim = patch_size\n",
    "        self.patch_dim = [patch_size, channels, patch_dim]\n",
    "        \n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (n p) -> b n (p c)', p = patch_size),\n",
    "            # batch_size channels (patch_number * patch_size) -> batch_size patch_number (patch_size * channels)\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "    def forward(self, series):\n",
    "        x = self.to_patch_embedding(series)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, 'd -> b d', b = b)\n",
    "        \n",
    "        x, ps = pack([cls_tokens, x], 'b * d')\n",
    "\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x, ps\n",
    "    \n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "                \n",
    "        self.to_qkv1 = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_qkv2 = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        qkv1 = self.to_qkv1(x).chunk(3, dim = -1)\n",
    "        _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv1)\n",
    "\n",
    "        qkv2 = self.to_qkv2(y).chunk(3, dim = -1)\n",
    "        q2, _, _ = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv2)\n",
    "        \n",
    "        dots = torch.matmul(q2, k1.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v1)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, lstm_hidden_dim=64, lstm_layers=2, dropout=0.1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_dim, lstm_hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.embedding = nn.Linear(lstm_hidden_dim*2, input_dim)\n",
    "        \n",
    "        self.mlp = MultiLayerPerceptron(dim=input_dim,\n",
    "                                        num_classes=num_classes,\n",
    "                                        dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM部分\n",
    "        x, _ = self.bilstm(x)  # x: (batch_size, seq_length, input_dim)\n",
    "        x = x[:, -1, :]  # 取最后一个时间步的输出作为特征\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        y = self.mlp(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### +Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTransformer(nn.Module):    \n",
    "    def __init__(self, input_dim, num_classes, lstm_hidden_dim=64, lstm_layers=2, nhead=4, dim_feedforward=128, num_layers=6, dropout=0.1):\n",
    "        super(BiLSTMTransformer, self).__init__()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_dim, lstm_hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(lstm_hidden_dim*2, input_dim)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.mlp = MultiLayerPerceptron(dim=input_dim,\n",
    "                                        num_classes=num_classes,\n",
    "                                        dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM部分\n",
    "        x, _ = self.bilstm(x)  # x: (batch_size, seq_length, input_dim)\n",
    "        x = x[:, -1, :]  # 取最后一个时间步的输出作为特征\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = self.transformer_encoder(x).squeeze(1)\n",
    "\n",
    "        y = self.mlp(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### +Inversely-proportional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTransformerGenerator(nn.Module):    \n",
    "    def __init__(self, z_dim, input_dim, num_classes, lstm_hidden_dim=64, lstm_layers=2, nhead=4, dim_feedforward=128, num_layers=6, dropout=0.1):\n",
    "        super(BiLSTMTransformerGenerator, self).__init__()\n",
    "        # Generator\n",
    "        self.input_noise = nn.Sequential(nn.Linear(z_dim, input_dim),\n",
    "                                         nn.LayerNorm(input_dim))\n",
    "        \n",
    "        self.input_label = nn.Sequential(nn.Linear(num_classes, input_dim),            \n",
    "                                         nn.LayerNorm(input_dim))\n",
    "        \n",
    "        self.multiheadcrossattention = MultiHeadCrossAttention(dim=input_dim, \n",
    "                                                               heads=nhead, \n",
    "                                                               dim_head = int(input_dim/nhead), \n",
    "                                                               dropout = dropout)\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_dim, lstm_hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(lstm_hidden_dim*2, input_dim)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.mlp = MultiLayerPerceptron(dim=input_dim,\n",
    "                                        num_classes=input_dim,\n",
    "                                        dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        noise = self.input_noise(x)\n",
    "        label = self.input_label(y)\n",
    "\n",
    "        mhca = self.multiheadcrossattention(noise, label)\n",
    "\n",
    "        # LSTM部分\n",
    "        x, _ = self.bilstm(mhca)  # x: (batch_size, seq_length, input_dim)\n",
    "        x = x[:, -1, :]  # 取最后一个时间步的输出作为特征\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = self.transformer_encoder(x).squeeze(1)\n",
    "\n",
    "        y = self.mlp(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数、模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoches = 30\n",
    "batch_size=256\n",
    "transformer_block_dim = 128\n",
    "last_epoch = -1\n",
    "print_interval = 100\n",
    "\n",
    "input_dim = 128\n",
    "z_dim = 100\n",
    "class_num = 5\n",
    "lstm_layers = 2\n",
    "nhead = 4\n",
    "tb_layers = 6\n",
    "dropout = 0.1\n",
    "\n",
    "# model = BiLSTM(input_dim=input_dim, \n",
    "#                num_classes=class_num, \n",
    "#                lstm_hidden_dim=input_dim, \n",
    "#                lstm_layers=lstm_layers,\n",
    "#                dropout=dropout).to(torch.device(\"cuda\"))\n",
    "\n",
    "dis = BiLSTMTransformer(\n",
    "                          input_dim=input_dim, \n",
    "                          num_classes=class_num, \n",
    "                          lstm_hidden_dim=input_dim, \n",
    "                          lstm_layers=lstm_layers, \n",
    "                          nhead=nhead, \n",
    "                          dim_feedforward=input_dim, \n",
    "                          num_layers=tb_layers, \n",
    "                          dropout=dropout\n",
    "                        ).to(torch.device(\"cuda\"))\n",
    "\n",
    "gen = BiLSTMTransformerGenerator(\n",
    "                          z_dim=z_dim,\n",
    "                          input_dim=input_dim, \n",
    "                          num_classes=class_num, \n",
    "                          lstm_hidden_dim=input_dim, \n",
    "                          lstm_layers=lstm_layers, \n",
    "                          nhead=nhead, \n",
    "                          dim_feedforward=input_dim, \n",
    "                          num_layers=tb_layers, \n",
    "                          dropout=dropout\n",
    "                        ).to(torch.device(\"cuda\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_dis = Adam(params=dis.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "optimizer_gen = Adam(params=gen.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler_dis = CosineAnnealingLR(optimizer_dis, T_max=train_epoches, last_epoch=last_epoch)\n",
    "scheduler_gen = CosineAnnealingLR(optimizer_gen, T_max=train_epoches, last_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data='datasets/NSL-KDD/KDDTrain+.txt'\n",
    "path_test_data='datasets/NSL-KDD/KDDTest+.txt'\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = Load_NSLKDD(path_train_data=path_train_data, \n",
    "                                               path_test_data=path_test_data, \n",
    "                                               binary_or_multi='multi')# 装载数据到loader里面\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "class_number_count = pd.DataFrame(Y_train).value_counts().values # 训练集数据Value Counts\n",
    "class_ratio = [item/sum(class_number_count) for item in class_number_count] # 训练集数据各类占比\n",
    "minus_log = [-math.log(item) for item in class_ratio] # 计算各类的倒数自然对数\n",
    "invert_ratio = [log/sum(minus_log) for log in minus_log] # 计算各类的反比\n",
    "\n",
    "def _weighted_random_int():\n",
    "    total = sum(invert_ratio)\n",
    "    r = random.uniform(0, total)\n",
    "    s = 0\n",
    "    for i, w in enumerate(invert_ratio):\n",
    "        s += w\n",
    "        if r < s:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch1\n",
      "Epo0/Iter100 - Accu:0.80859375\n",
      "Epo0/Iter200 - Accu:0.90234375\n",
      "Epo0/Iter300 - Accu:0.91796875\n",
      "Epo0/Iter400 - Accu:0.9453125\n",
      "Epoch 1/30:\n",
      "Accuracy:0.7290631653655074\n",
      "Best Accuracy:0.7290631653655074 - Epoch:1\n",
      "\n",
      "Training Epoch2\n",
      "Epo1/Iter100 - Accu:0.91015625\n",
      "Epo1/Iter200 - Accu:0.9375\n",
      "Epo1/Iter300 - Accu:0.93359375\n",
      "Epo1/Iter400 - Accu:0.94921875\n",
      "Epoch 2/30:\n",
      "Accuracy:0.7244943222143364\n",
      "Best Accuracy:0.7290631653655074 - Epoch:1\n",
      "\n",
      "Training Epoch3\n",
      "Epo2/Iter100 - Accu:0.90625\n",
      "Epo2/Iter200 - Accu:0.9453125\n",
      "Epo2/Iter300 - Accu:0.90625\n",
      "Epo2/Iter400 - Accu:0.921875\n",
      "Epoch 3/30:\n",
      "Accuracy:0.7268452803406671\n",
      "Best Accuracy:0.7290631653655074 - Epoch:1\n",
      "\n",
      "Training Epoch4\n",
      "Epo3/Iter100 - Accu:0.91796875\n",
      "Epo3/Iter200 - Accu:0.93359375\n",
      "Epo3/Iter300 - Accu:0.95703125\n",
      "Epo3/Iter400 - Accu:0.90625\n",
      "Epoch 4/30:\n",
      "Accuracy:0.7264904187366927\n",
      "Best Accuracy:0.7290631653655074 - Epoch:1\n",
      "\n",
      "Training Epoch5\n",
      "Epo4/Iter100 - Accu:0.9296875\n",
      "Epo4/Iter200 - Accu:0.9375\n",
      "Epo4/Iter300 - Accu:0.93359375\n",
      "Epo4/Iter400 - Accu:0.94921875\n",
      "Epoch 5/30:\n",
      "Accuracy:0.7246273953158269\n",
      "Best Accuracy:0.7290631653655074 - Epoch:1\n",
      "\n",
      "Training Epoch6\n",
      "Epo5/Iter100 - Accu:0.921875\n",
      "Epo5/Iter200 - Accu:0.9296875\n",
      "Epo5/Iter300 - Accu:0.9296875\n",
      "Epo5/Iter400 - Accu:0.94921875\n",
      "Epoch 6/30:\n",
      "Accuracy:0.7264904187366927\n",
      "Best Accuracy:0.7290631653655074 - Epoch:1\n",
      "\n",
      "Training Epoch7\n",
      "Epo6/Iter100 - Accu:0.8984375\n",
      "Epo6/Iter200 - Accu:0.9453125\n",
      "Epo6/Iter300 - Accu:0.94921875\n",
      "Epo6/Iter400 - Accu:0.93359375\n",
      "Epoch 7/30:\n",
      "Accuracy:0.7196149751596878\n",
      "Best Accuracy:0.7290631653655074 - Epoch:1\n",
      "\n",
      "Training Epoch8\n",
      "Epo7/Iter100 - Accu:0.9296875\n",
      "Epo7/Iter200 - Accu:0.953125\n",
      "Epo7/Iter300 - Accu:0.92578125\n",
      "Epo7/Iter400 - Accu:0.94921875\n",
      "Epoch 8/30:\n",
      "Accuracy:0.7252040454222853\n",
      "Best Accuracy:0.7290631653655074 - Epoch:1\n",
      "\n",
      "Training Epoch9\n",
      "Epo8/Iter100 - Accu:0.9296875\n",
      "Epo8/Iter200 - Accu:0.94140625\n",
      "Epo8/Iter300 - Accu:0.91796875\n",
      "Epo8/Iter400 - Accu:0.94921875\n",
      "Epoch 9/30:\n",
      "Accuracy:0.7394872249822569\n",
      "Best Accuracy:0.7394872249822569 - Epoch:9\n",
      "\n",
      "Training Epoch10\n",
      "Epo9/Iter100 - Accu:0.94140625\n",
      "Epo9/Iter200 - Accu:0.91796875\n",
      "Epo9/Iter300 - Accu:0.921875\n",
      "Epo9/Iter400 - Accu:0.9296875\n",
      "Epoch 10/30:\n",
      "Accuracy:0.7264904187366927\n",
      "Best Accuracy:0.7394872249822569 - Epoch:9\n",
      "\n",
      "Training Epoch11\n",
      "Epo10/Iter100 - Accu:0.94140625\n",
      "Epo10/Iter200 - Accu:0.96484375\n",
      "Epo10/Iter300 - Accu:0.9296875\n",
      "Epo10/Iter400 - Accu:0.92578125\n",
      "Epoch 11/30:\n",
      "Accuracy:0.7262242725337118\n",
      "Best Accuracy:0.7394872249822569 - Epoch:9\n",
      "\n",
      "Training Epoch12\n",
      "Epo11/Iter100 - Accu:0.9375\n",
      "Epo11/Iter200 - Accu:0.95703125\n",
      "Epo11/Iter300 - Accu:0.9453125\n",
      "Epo11/Iter400 - Accu:0.93359375\n",
      "Epoch 12/30:\n",
      "Accuracy:0.7310592618878637\n",
      "Best Accuracy:0.7394872249822569 - Epoch:9\n",
      "\n",
      "Training Epoch13\n",
      "Epo12/Iter100 - Accu:0.9296875\n",
      "Epo12/Iter200 - Accu:0.94921875\n",
      "Epo12/Iter300 - Accu:0.94140625\n",
      "Epo12/Iter400 - Accu:0.9375\n",
      "Epoch 13/30:\n",
      "Accuracy:0.7292405961674947\n",
      "Best Accuracy:0.7394872249822569 - Epoch:9\n",
      "\n",
      "Training Epoch14\n",
      "Epo13/Iter100 - Accu:0.9140625\n",
      "Epo13/Iter200 - Accu:0.9609375\n",
      "Epo13/Iter300 - Accu:0.9296875\n",
      "Epo13/Iter400 - Accu:0.921875\n",
      "Epoch 14/30:\n",
      "Accuracy:0.7303938963804116\n",
      "Best Accuracy:0.7394872249822569 - Epoch:9\n",
      "\n",
      "Training Epoch15\n",
      "Epo14/Iter100 - Accu:0.9375\n",
      "Epo14/Iter200 - Accu:0.921875\n",
      "Epo14/Iter300 - Accu:0.9296875\n",
      "Epo14/Iter400 - Accu:0.953125\n",
      "Epoch 15/30:\n",
      "Accuracy:0.7438786373314408\n",
      "Best Accuracy:0.7438786373314408 - Epoch:15\n",
      "\n",
      "Training Epoch16\n",
      "Epo15/Iter100 - Accu:0.953125\n",
      "Epo15/Iter200 - Accu:0.92578125\n",
      "Epo15/Iter300 - Accu:0.95703125\n",
      "Epo15/Iter400 - Accu:0.9296875\n",
      "Epoch 16/30:\n",
      "Accuracy:0.7370031937544358\n",
      "Best Accuracy:0.7438786373314408 - Epoch:15\n",
      "\n",
      "Training Epoch17\n",
      "Epo16/Iter100 - Accu:0.953125\n",
      "Epo16/Iter200 - Accu:0.94140625\n",
      "Epo16/Iter300 - Accu:0.94140625\n",
      "Epo16/Iter400 - Accu:0.9453125\n",
      "Epoch 17/30:\n",
      "Accuracy:0.7494233498935415\n",
      "Best Accuracy:0.7494233498935415 - Epoch:17\n",
      "\n",
      "Training Epoch18\n",
      "Epo17/Iter100 - Accu:0.9453125\n",
      "Epo17/Iter200 - Accu:0.95703125\n",
      "Epo17/Iter300 - Accu:0.94921875\n",
      "Epo17/Iter400 - Accu:0.95703125\n",
      "Epoch 18/30:\n",
      "Accuracy:0.7488910574875799\n",
      "Best Accuracy:0.7494233498935415 - Epoch:17\n",
      "\n",
      "Training Epoch19\n",
      "Epo18/Iter100 - Accu:0.94921875\n",
      "Epo18/Iter200 - Accu:0.9609375\n",
      "Epo18/Iter300 - Accu:0.953125\n",
      "Epo18/Iter400 - Accu:0.9296875\n",
      "Epoch 19/30:\n",
      "Accuracy:0.7546132008516678\n",
      "Best Accuracy:0.7546132008516678 - Epoch:19\n",
      "\n",
      "Training Epoch20\n",
      "Epo19/Iter100 - Accu:0.9453125\n",
      "Epo19/Iter200 - Accu:0.953125\n",
      "Epo19/Iter300 - Accu:0.953125\n",
      "Epo19/Iter400 - Accu:0.94921875\n",
      "Epoch 20/30:\n",
      "Accuracy:0.7551898509581263\n",
      "Best Accuracy:0.7551898509581263 - Epoch:20\n",
      "\n",
      "Training Epoch21\n",
      "Epo20/Iter100 - Accu:0.95703125\n",
      "Epo20/Iter200 - Accu:0.9609375\n",
      "Epo20/Iter300 - Accu:0.93359375\n",
      "Epo20/Iter400 - Accu:0.9453125\n",
      "Epoch 21/30:\n",
      "Accuracy:0.7482700496806246\n",
      "Best Accuracy:0.7551898509581263 - Epoch:20\n",
      "\n",
      "Training Epoch22\n",
      "Epo21/Iter100 - Accu:0.953125\n",
      "Epo21/Iter200 - Accu:0.95703125\n",
      "Epo21/Iter300 - Accu:0.9375\n",
      "Epo21/Iter400 - Accu:0.953125\n",
      "Epoch 22/30:\n",
      "Accuracy:0.7455642299503193\n",
      "Best Accuracy:0.7551898509581263 - Epoch:20\n",
      "\n",
      "Training Epoch23\n",
      "Epo22/Iter100 - Accu:0.9609375\n",
      "Epo22/Iter200 - Accu:0.9375\n",
      "Epo22/Iter300 - Accu:0.93359375\n",
      "Epo22/Iter400 - Accu:0.9296875\n",
      "Epoch 23/30:\n",
      "Accuracy:0.7638396025550035\n",
      "Best Accuracy:0.7638396025550035 - Epoch:23\n",
      "\n",
      "Training Epoch24\n",
      "Epo23/Iter100 - Accu:0.92578125\n",
      "Epo23/Iter200 - Accu:0.94140625\n",
      "Epo23/Iter300 - Accu:0.9375\n",
      "Epo23/Iter400 - Accu:0.96484375\n",
      "Epoch 24/30:\n",
      "Accuracy:0.7534155429382541\n",
      "Best Accuracy:0.7638396025550035 - Epoch:23\n",
      "\n",
      "Training Epoch25\n",
      "Epo24/Iter100 - Accu:0.96484375\n",
      "Epo24/Iter200 - Accu:0.9296875\n",
      "Epo24/Iter300 - Accu:0.9453125\n",
      "Epo24/Iter400 - Accu:0.95703125\n",
      "Epoch 25/30:\n",
      "Accuracy:0.7510645848119234\n",
      "Best Accuracy:0.7638396025550035 - Epoch:23\n",
      "\n",
      "Training Epoch26\n",
      "Epo25/Iter100 - Accu:0.9453125\n",
      "Epo25/Iter200 - Accu:0.9453125\n",
      "Epo25/Iter300 - Accu:0.953125\n",
      "Epo25/Iter400 - Accu:0.95703125\n",
      "Epoch 26/30:\n",
      "Accuracy:0.7397533711852378\n",
      "Best Accuracy:0.7638396025550035 - Epoch:23\n",
      "\n",
      "Training Epoch27\n",
      "Epo26/Iter100 - Accu:0.94921875\n",
      "Epo26/Iter200 - Accu:0.94140625\n",
      "Epo26/Iter300 - Accu:0.95703125\n",
      "Epo26/Iter400 - Accu:0.93359375\n",
      "Epoch 27/30:\n",
      "Accuracy:0.7505766501064585\n",
      "Best Accuracy:0.7638396025550035 - Epoch:23\n",
      "\n",
      "Training Epoch28\n",
      "Epo27/Iter100 - Accu:0.9609375\n",
      "Epo27/Iter200 - Accu:0.9375\n",
      "Epo27/Iter300 - Accu:0.92578125\n",
      "Epo27/Iter400 - Accu:0.9140625\n",
      "Epoch 28/30:\n",
      "Accuracy:0.7515525195173882\n",
      "Best Accuracy:0.7638396025550035 - Epoch:23\n",
      "\n",
      "Training Epoch29\n",
      "Epo28/Iter100 - Accu:0.94921875\n",
      "Epo28/Iter200 - Accu:0.953125\n",
      "Epo28/Iter300 - Accu:0.94140625\n",
      "Epo28/Iter400 - Accu:0.953125\n",
      "Epoch 29/30:\n",
      "Accuracy:0.7365152590489709\n",
      "Best Accuracy:0.7638396025550035 - Epoch:23\n",
      "\n",
      "Training Epoch30\n",
      "Epo29/Iter100 - Accu:0.9296875\n",
      "Epo29/Iter200 - Accu:0.9375\n",
      "Epo29/Iter300 - Accu:0.953125\n",
      "Epo29/Iter400 - Accu:0.9375\n",
      "Epoch 30/30:\n",
      "Accuracy:0.7523953158268275\n",
      "Best Accuracy:0.7638396025550035 - Epoch:23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accu = 0\n",
    "best_epoch = 0\n",
    "for epo in range(train_epoches):\n",
    "    print(f'Training Epoch{epo+1}')    \n",
    "    \n",
    "    for index, (train_image, train_label) in enumerate(train_loader):\n",
    "        train_image, train_label = train_image.view(train_image.shape[0], -1, train_image.shape[1]).cuda(non_blocking=True), train_label.to(torch.int64).cuda(non_blocking=True)\n",
    "        train_label = torch.nn.functional.one_hot(train_label, class_num).to(torch.float32).cuda(non_blocking=True)\n",
    "        optimizer_dis.zero_grad()\n",
    "        #############\n",
    "        # 训练判别器\n",
    "        #############\n",
    "        dis.train()\n",
    "        outs_real = dis(train_image).view(train_label.shape)        \n",
    "        dis_loss = criterion(outs_real, train_label)\n",
    "\n",
    "        dis_loss.backward()\n",
    "        optimizer_dis.step()\n",
    "        scheduler_dis.step()\n",
    "\n",
    "        #############\n",
    "        # 训练生成器\n",
    "        #############\n",
    "        gen.train()\n",
    "        # dis.eval()\n",
    "        # 生成噪声\n",
    "        noise = torch.randn(train_image.shape[0], 1, z_dim).cuda(non_blocking=True)\n",
    "        \n",
    "        # 生成反比例标签\n",
    "        inversely_label = np.array([_weighted_random_int() for i in range(train_image.shape[0])])\n",
    "        fake_label = torch.from_numpy(inversely_label).to(int).cuda(non_blocking=True)\n",
    "        fake_label_one_hot = nn.functional.one_hot(fake_label, num_classes=class_num).to(torch.float32).view(train_image.shape[0], 1, -1).cuda(non_blocking=True)\n",
    "            \n",
    "        fake_sample = gen(noise, fake_label_one_hot).unsqueeze(1)\n",
    "        # with torch.no_grad():\n",
    "        #     generate_outs = dis(fake_sample).view(train_label.shape)\n",
    "        generate_outs = dis(fake_sample).view(train_label.shape)\n",
    "        gen_loss = criterion(generate_outs, fake_label_one_hot.squeeze(1))\n",
    "        gen_loss.backward()\n",
    "        optimizer_gen.step()\n",
    "        scheduler_gen.step()\n",
    "        \n",
    "        \n",
    "        accuracy = accuracy_score(torch.argmax(outs_real, dim=1).cpu(), torch.argmax(train_label, dim=1).cpu())\n",
    "        if (index % print_interval) == 0 and index != 0:\n",
    "            print(f'Epo{epo}/Iter{index} - Accu:{accuracy}')\n",
    "\n",
    "    dis.eval()    \n",
    "    Y_pred = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    for image, label in test_loader:\n",
    "        Y_test = np.append(Y_test, label, axis=None)\n",
    "        image, label = image.view(image.shape[0], -1, image.shape[1]).cuda(non_blocking=True), label.cuda(non_blocking=True)\n",
    "\n",
    "        predicted = dis(image).view(label.shape[0], -1)\n",
    "        predicted = torch.argmax(predicted, dim=1)\n",
    "        \n",
    "        Y_pred = np.append(Y_pred, predicted.cpu().numpy(), axis=None)\n",
    "            \n",
    "    accuracy_test = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    if accuracy_test > best_accu:\n",
    "        best_accu = accuracy_test\n",
    "        best_epoch = epo\n",
    "        best_pred = Y_pred\n",
    "        \n",
    "        #保存整个模型\n",
    "        torch.save(dis, f'{save_path}/Inversely-proportional_Generation/models/BiLSTMTransformer_{epo}_accu_{accuracy_test:.4f}.pth')\n",
    "        \n",
    "    print_lines = f'Epoch {epo+1}/{train_epoches}:\\nAccuracy:{accuracy_test}\\n'\n",
    "    print_lines += f'Best Accuracy:{best_accu} - Epoch:{best_epoch+1}\\n'\n",
    "    print(print_lines)\n",
    "    \n",
    "    with open(f'{save_path}/Inversely-proportional_Generation/results/Inversely-proportional_Generation_Multi.txt', 'a', encoding='utf-8') as file:\n",
    "        # 将输出内容写入文件\n",
    "        file.write(print_lines+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
